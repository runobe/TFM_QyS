{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc9abe7",
   "metadata": {},
   "source": [
    "<h3> Palabras (TF-IDF)</h3>\n",
    "<ol><li>Calcular el promedio de TF-IDF para cada palabra en el clúster\n",
    "        tfidf_promedio = cluster_tfidf.mean(axis=0).A1  #A1 convierte la matriz sparse a un array 1D eficiente</li>\n",
    "<li> los índices de las n palabras con la mayor puntuación TF-IDF promedio\n",
    "        palabras_representativas_indices = np.argsort(tfidf_promedio)[::-1][:n]</li></ol>\n",
    "<h3>Documentos (TF-IDF) a nivel de clúster</h3>\n",
    "<ol><li>Calcular Tf a nivel de clúster</li>\n",
    "<li>Calcular IDF a nivel de clúster</li>\n",
    "<li>Generar puntuaciones para cada documento del clúster </li></ol>\n",
    "<h3> Palabras (Word2Vec)</h3>\n",
    "<ol><li>Obtener embeddings promedio de palabras por documento\n",
    "        cluster_vectores = []\n",
    "        for doc in cluster_documentos:\n",
    "            palabra_vectors = [model_w2v.wv[palabra] for palabra in doc if palabra in model_w2v.wv]\n",
    "            if palabra_vectors:  # Evitar documentos vacíos\n",
    "                cluster_vectores.append(np.mean(palabra_vectors, axis=0))</li>\n",
    "\n",
    "<li> Calcular el promedio del clúster\n",
    "        cluster_centroide = np.mean(cluster_vectores, axis=0)</li>\n",
    "<li> Encontrar palabras más similar al promedio\n",
    "        palabras_cercanas = model_w2v.wv.similar_by_vector(cluster_centroide, topn=n)</li></ol>\n",
    "<h3> Documentos (Word2Vec)</h3>\n",
    "    <ol><li> Aplicar la misma función que para TF-IDF</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6716d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import bz2\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import ast\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a3eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recursos/resultados_modelos.pkl', 'rb') as archivo:\n",
    "    resultados = pickle.load(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d0d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas_predichas=resultados[\"agglomerative_Tfidf\"]['etiquetas_predichas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626c48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recursos/modelos_topicos/agglomerative_Tfidf_ejecucion_0.pkl', 'rb') as archivo:\n",
    "    modelo = pickle.load(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383955a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_documents_for_dictionary(tokenized_documents, filtered_words):\n",
    "    filtered_documents = []\n",
    "    for doc in tokenized_documents:\n",
    "        filtered_doc = [word for word in doc if word in filtered_words]\n",
    "        filtered_documents.append(filtered_doc)\n",
    "    return filtered_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34364d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_preprocesado = pd.read_csv('df_preprocesado.csv', sep=';')\n",
    "dictionary = Dictionary.load('recursos/dictionary_filtrado_spacy.gensim')\n",
    "filtered_words = set(dictionary.token2id.keys())\n",
    "n_term=len(filtered_words)\n",
    "#preprocesado_documents=df_preprocesado['texto_preprocesado'].apply(ast.literal_eval)    \n",
    "#documents = filter_documents_for_dictionary(preprocesado_documents, filtered_words) \n",
    "with open('recursos/documents.pkl', 'rb') as archivo:\n",
    "    documents = pickle.load(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a14f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_como_cadenas = [\" \".join(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c216742",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "matriz_tfidf = vectorizer.fit_transform(documentos_como_cadenas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b3372c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('matriz_tfidf.pkl', 'wb') as archivo_tfidf:\n",
    "    pickle.dump(matriz_tfidf, archivo_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51ca8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0eead42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def obtener_clusters_ordenados(etiquetas):\n",
    "       # Contar el número de documentos por clúster\n",
    "    conteo_clusters = Counter(etiquetas)\n",
    "\n",
    "    # Eliminar el clúster residual (-1)\n",
    "    del conteo_clusters[-1]\n",
    "\n",
    "    # Ordenar por número de documentos (de mayor a menor)\n",
    "    clusters_ordenados = sorted(conteo_clusters.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return clusters_ordenados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a341c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_palabras_representativas_tfidf(matriz_tfidf, etiquetas, vocabulario, n=15,n_top_clusters=20, output_excel_path=\"palabras_representativas_agg_tfidf.xlsx\"):\n",
    "   \n",
    "    cluster_palabras_representativas = {}\n",
    "\n",
    "# Obtener los clústeres ordenados por el número de documentos\n",
    "    clusters_ordenados = obtener_clusters_ordenados(etiquetas)\n",
    "\n",
    "    # Tomar los top N clústeres\n",
    "    top_clusters = clusters_ordenados[:n_top_clusters]\n",
    "\n",
    "    # Crear una lista para almacenar los datos de las palabras y su clúster\n",
    "    data = []    \n",
    "    \n",
    "    for cluster_label, _ in top_clusters:\n",
    "        # Obtener los documentos que pertenecen al clúster\n",
    "        cluster_indices = np.where(etiquetas == cluster_label)[0]\n",
    "        cluster_tfidf = matriz_tfidf[cluster_indices]\n",
    "\n",
    "        # Verificar que haya documentos en el clúster\n",
    "        if cluster_tfidf.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # Calcular el promedio de TF-IDF para cada palabra en el clúster\n",
    "        tfidf_promedio = cluster_tfidf.mean(axis=0).A1  #A1 convierte la matriz sparse a un array 1D eficiente\n",
    "\n",
    "        # Obtener los índices de las n palabras con la mayor puntuación TF-IDF promedio\n",
    "        palabras_representativas_indices = np.argsort(tfidf_promedio)[::-1][:n]\n",
    "\n",
    "        # Obtener las palabras representativas\n",
    "        palabras_representativas = [vocabulario[i] for i in palabras_representativas_indices]\n",
    "\n",
    "         # Unir las palabras representativas en una sola cadena separada por comas\n",
    "        palabras_string = \", \".join(palabras_representativas)\n",
    "\n",
    "        # Añadir el ID del clúster y las palabras al listado\n",
    "        data.append({\"Cluster ID\": cluster_label, \"Palabras\": palabras_string})\n",
    "\n",
    "    # Crear un DataFrame de pandas\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Exportar a Excel\n",
    "    df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "    print(f\"Las palabras representativas de los clústeres se han exportado a '{output_excel_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af054faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las palabras representativas de los clústeres se han exportado a 'palabras_representativas_agg_tfidf.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "obtener_palabras_representativas_tfidf(matriz_tfidf, etiquetas_predichas, vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9e91c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documentos_originales=df_preprocesado['texto']\n",
    "with bz2.BZ2File('recursos/documentos_originales.pkl.bz2', 'rb') as f:\n",
    "    documentos_originales = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5069f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def imprimir_documentos_relevantes_clusters(etiquetas_predichas, documentos_preprocesados, documentos_originales, top_index, num_documentos=5, nombre_archivo=\"documentos_relevantes.xlsx\"):\n",
    "    \n",
    "    resultados_totales = []\n",
    " \n",
    "    for cluster_id in top_index:\n",
    "        documentos_cluster = [(i, documentos_preprocesados[i]) for i, etiqueta in enumerate(etiquetas_predichas) if etiqueta == cluster_id]\n",
    "\n",
    "        # Calcular TF\n",
    "        tf_cluster = defaultdict(int)\n",
    "        for _, documento in documentos_cluster:\n",
    "            for palabra in documento:\n",
    "                tf_cluster[palabra] += 1\n",
    "\n",
    "        # Calcular IDF\n",
    "        palabras_por_cluster = defaultdict(set)\n",
    "        for i, etiqueta in enumerate(etiquetas_predichas):\n",
    "            palabras_por_cluster[etiqueta].update(documentos_preprocesados[i])\n",
    "\n",
    "        num_clusters = len(set(etiquetas_predichas))\n",
    "        num_clusters_con_palabra = {}\n",
    "        idf_cluster = {}\n",
    "        peso_palabras = {}\n",
    "        for palabra in tf_cluster:\n",
    "            contador = 0\n",
    "            for palabras in palabras_por_cluster.values():\n",
    "                if palabra in palabras:\n",
    "                    contador += 1\n",
    "            num_clusters_con_palabra[palabra] = contador\n",
    "            frecuencia = num_clusters_con_palabra[palabra]\n",
    "            idf = np.log((1 + num_clusters) / (1 + frecuencia)) + 1\n",
    "            idf_cluster[palabra] = idf\n",
    "            peso_palabras[palabra] = tf_cluster[palabra] * idf_cluster[palabra]\n",
    "\n",
    "        # Evaluar documentos del clúster\n",
    "        puntuaciones_relevancia = []\n",
    "        for doc_id, documento in documentos_cluster:\n",
    "            # Calcular la frecuencia relativa de cada palabra\n",
    "            total_palabras_doc = len(documento)\n",
    "            puntuacion = np.array([peso_palabras.get(palabra, 0) * (documento.count(palabra) / total_palabras_doc) for palabra in documento])\n",
    "            puntuacion_final = sum(puntuacion)\n",
    "\n",
    "            # Cortar los primeros 300 caracteres del documento original\n",
    "            documento_recortado = documentos_originales[doc_id][:300]\n",
    "\n",
    "            puntuaciones_relevancia.append((doc_id, cluster_id, documento_recortado, puntuacion_final))\n",
    "\n",
    "        # Ordenar por puntuación\n",
    "        puntuaciones_relevancia_ordenadas = sorted(puntuaciones_relevancia, key=lambda x: x[3], reverse=True)[:num_documentos]\n",
    "\n",
    "        # Añadir resultados de este cluster\n",
    "        resultados_totales.extend(puntuaciones_relevancia_ordenadas)\n",
    "\n",
    "    # Verificar que hay datos antes de crear el DataFrame\n",
    "    if resultados_totales:\n",
    "        df = pd.DataFrame(resultados_totales, columns=[\"ID\", \"ID de Cluster\", \"Documento (200 caracteres)\", \"Puntuación\"])\n",
    "\n",
    "        # Guardar en formato CSV, XLSX o ODS según el nombre de archivo\n",
    "        if nombre_archivo.endswith('.csv'):\n",
    "            df.to_csv(nombre_archivo, index=False, encoding=\"utf-8\")\n",
    "        elif nombre_archivo.endswith('.ods'):\n",
    "            df.to_excel(nombre_archivo, index=False, engine=\"odf\")\n",
    "        else:\n",
    "            df.to_excel(nombre_archivo, index=False, engine=\"openpyxl\")\n",
    "        print(f\" Archivo guardado: {nombre_archivo}\")\n",
    "    else:\n",
    "        print(\"No hay documentos relevantes para los clusters indicados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8849b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_originales=df_preprocesado['texto']\n",
    "# Obtener los clústeres ordenados por el número de documentos\n",
    "clusters_ordenados = obtener_clusters_ordenados(etiquetas_predichas)\n",
    "\n",
    "# Tomar los top N clústeres\n",
    "top_clusters = clusters_ordenados[:20]\n",
    "top_clusters_ids = [cluster_id for cluster_id, _ in top_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ad287",
   "metadata": {},
   "outputs": [],
   "source": [
    "imprimir_documentos_relevantes_clusters(etiquetas_predichas, documents, documentos_originales, top_clusters_ids, num_documentos=5, nombre_archivo=\"documentos_relevantes_agglo_tfidf.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf66f4",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ee0fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recursos/modelos_topicos/agglomerative_word2vec_ejecucion_0.pkl', 'rb') as archivo:\n",
    "    modelo = pickle.load(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "823ad5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas_predichas=resultados[\"agglomerative_word2vec\"]['etiquetas_predichas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb50d5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 23, 11, ...,  7, 20,  7], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etiquetas_predichas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7ea54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def obtener_palabras_representativas_w2v(model_w2v, documents, labels, n=10):\n",
    "   \n",
    "\n",
    "    cluster_palabras_representativas = {}\n",
    "\n",
    "    for cluster_label in np.unique(labels):\n",
    "        cluster_indices = np.where(labels == cluster_label)[0]\n",
    "        cluster_documentos = [documents[i] for i in cluster_indices]\n",
    "\n",
    "        # Obtener embeddings promedio de palabras por documento\n",
    "        cluster_vectores = []\n",
    "        for doc in cluster_documentos:\n",
    "            palabra_vectors = [model_w2v.wv[palabra] for palabra in doc if palabra in model_w2v.wv]\n",
    "            if palabra_vectors:  # Evitar documentos vacíos\n",
    "                cluster_vectores.append(np.mean(palabra_vectors, axis=0))\n",
    "\n",
    "        if not cluster_vectores:\n",
    "            cluster_palabras_representativas[cluster_label] = []\n",
    "            continue\n",
    "\n",
    "        # Calcular el promedio del clúster\n",
    "        cluster_centroide = np.mean(cluster_vectores, axis=0)\n",
    "\n",
    "        # Encontrar palabras más similar al promedio\n",
    "        try:\n",
    "            palabras_cercanas = model_w2v.wv.similar_by_vector(cluster_centroide, topn=n)\n",
    "            cluster_palabras_representativas[cluster_label] = [palabra for palabra, _ in palabras_cercanas]\n",
    "        except:\n",
    "            cluster_palabras_representativas[cluster_label] = []\n",
    "\n",
    "    return cluster_palabras_representativas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c644f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_documents_for_dictionary(tokenized_documents, filtered_words):\n",
    "    filtered_documents = []\n",
    "    for doc in tokenized_documents:\n",
    "        filtered_doc = [word for word in doc if word in filtered_words]\n",
    "        filtered_documents.append(filtered_doc)\n",
    "    return filtered_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8765b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preprocesado = pd.read_csv('df_preprocesado.csv', sep=\";\")\n",
    "# dictionary = Dictionary.load('recursos/dictionary_filtrado_spacy.gensim')\n",
    "# filtered_words = set(dictionary.token2id.keys())\n",
    "# preprocesado_documents=df_preprocesado['texto_preprocesado'].apply(ast.literal_eval)\n",
    "# Generar documentos en igual de condiciones\n",
    "# documents = filter_documents_for_dictionary(preprocesado_documents,filtered_words)\n",
    "with open('recursos/documents.pkl', 'rb') as archivo:\n",
    "    documents = pickle.load(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01e36f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def obtener_palabras_representativas_w2v(model_w2v, documents, labels, n=15,n_top_clusters=20,output_excel_path=\"palabras_representativas_agglo_w2v.xlsx\"):\n",
    "   \n",
    "\n",
    "    # Obtener los clústeres ordenados por el número de documentos\n",
    "    clusters_ordenados = obtener_clusters_ordenados(labels)\n",
    "\n",
    "    # Tomar los top N clústeres\n",
    "    top_clusters = clusters_ordenados[:n_top_clusters]\n",
    "    cluster_palabras_representativas = {}\n",
    "# Crear una lista para almacenar los datos de las palabras y su clúster\n",
    "    data = []\n",
    "    for cluster_label, _ in top_clusters:\n",
    "        cluster_indices = np.where(labels == cluster_label)[0]\n",
    "        cluster_documentos = [documents[i] for i in cluster_indices]\n",
    "\n",
    "        # Obtener embeddings promedio de palabras por documento\n",
    "        cluster_vectores = []\n",
    "        for doc in cluster_documentos:\n",
    "            palabra_vectors = [model_w2v.wv[palabra] for palabra in doc if palabra in model_w2v.wv]\n",
    "            if palabra_vectors:  # Evitar documentos vacíos\n",
    "                cluster_vectores.append(np.mean(palabra_vectors, axis=0))\n",
    "\n",
    "        if not cluster_vectores:\n",
    "            cluster_palabras_representativas[cluster_label] = []\n",
    "            continue\n",
    "\n",
    "        # Calcular el promedio del clúster\n",
    "        cluster_centroide = np.mean(cluster_vectores, axis=0)\n",
    "\n",
    "        # Encontrar palabras más similar al promedio\n",
    "        try:\n",
    "            palabras_cercanas = model_w2v.wv.similar_by_vector(cluster_centroide, topn=n)\n",
    "            cluster_palabras_representativas[cluster_label] = [palabra for palabra, _ in palabras_cercanas]\n",
    "        except:\n",
    "            cluster_palabras_representativas[cluster_label] = []\n",
    "\n",
    "    # Unir las palabras representativas en una sola cadena separada por comas\n",
    "        palabras_string = \", \".join(cluster_palabras_representativas[cluster_label])\n",
    "\n",
    "        # Añadir el ID del clúster y las palabras al listado\n",
    "        data.append({\"Cluster ID\": cluster_label, \"Palabras\": palabras_string})\n",
    "\n",
    "    # Crear un DataFrame de pandas\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Exportar a Excel\n",
    "    df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "    print(f\"Las palabras representativas de los clústeres se han exportado a '{output_excel_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5f444aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las palabras representativas de los clústeres se han exportado a 'palabras_representativas_agglo_w2v.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo Word2Vec y las etiquetas predichas\n",
    "\n",
    "modelo_w2v_cargado = Word2Vec.load(\"recursos/w2v_250_epochs.model\")\n",
    "\n",
    "\n",
    "# Obtener las palabras representativas\n",
    "palabras_representativas = obtener_palabras_representativas_w2v(modelo_w2v_cargado, documents, etiquetas_predichas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88450596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preprocesado = pd.read_csv('df_preprocesado.csv', sep=';')\n",
    "# documentos_originales=df_preprocesado['texto']\n",
    "with bz2.BZ2File('recursos/documentos_originales.pkl.bz2', 'rb') as f:\n",
    "    documentos_originales = pickle.load(f)\n",
    "# Obtener los clústeres ordenados por el número de documentos\n",
    "clusters_ordenados = obtener_clusters_ordenados(etiquetas_predichas)\n",
    "\n",
    "# Tomar los top N clústeres\n",
    "top_clusters = clusters_ordenados[:20]\n",
    "top_clusters_ids = [cluster_id for cluster_id, _ in top_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de66443",
   "metadata": {},
   "outputs": [],
   "source": [
    "imprimir_documentos_relevantes_clusters(etiquetas_predichas, documents, documentos_originales, top_clusters_ids, num_documentos=5, nombre_archivo=\"documentos_relevantes_agglo_word2vec.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
